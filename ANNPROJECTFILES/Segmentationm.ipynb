{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0935ebc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (5, 26) was passed for an output of shape (None, 39) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredients_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Generate new cocktail recipes\u001b[39;00m\n\u001b[0;32m     47\u001b[0m input_ingredients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvodka\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcranberry juice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlime juice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:854\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    853\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:698\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    676\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    693\u001b[0m ):\n\u001b[0;32m    694\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(\n\u001b[0;32m    695\u001b[0m         batch_size, steps_per_epoch, x\n\u001b[0;32m    696\u001b[0m     )\n\u001b[1;32m--> 698\u001b[0m     x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validation_data:\n\u001b[0;32m    712\u001b[0m         val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_prepare_validation_data(\n\u001b[0;32m    713\u001b[0m             validation_data, batch_size, validation_steps\n\u001b[0;32m    714\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2650\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2642\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2643\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2646\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)\n\u001b[0;32m   2647\u001b[0m ):\n\u001b[0;32m   2648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2781\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2777\u001b[0m         training_utils_v1\u001b[38;5;241m.\u001b[39mcheck_array_lengths(x, y, sample_weights)\n\u001b[0;32m   2778\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run_eagerly:\n\u001b[0;32m   2779\u001b[0m             \u001b[38;5;66;03m# Additional checks to avoid users mistakenly using improper\u001b[39;00m\n\u001b[0;32m   2780\u001b[0m             \u001b[38;5;66;03m# loss fns.\u001b[39;00m\n\u001b[1;32m-> 2781\u001b[0m             \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_loss_and_target_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2782\u001b[0m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed_loss_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_output_shapes\u001b[49m\n\u001b[0;32m   2783\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2785\u001b[0m     sample_weights, _, _ \u001b[38;5;241m=\u001b[39m training_utils\u001b[38;5;241m.\u001b[39mhandle_partial_sample_weights(\n\u001b[0;32m   2786\u001b[0m         y, sample_weights, feed_sample_weight_modes, check_all_flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2787\u001b[0m     )\n\u001b[0;32m   2788\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:945\u001b[0m, in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    943\u001b[0m     loss_type \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;28;01mif\u001b[39;00m is_loss_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(loss)\n\u001b[0;32m    944\u001b[0m     loss_name \u001b[38;5;241m=\u001b[39m loss_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 945\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA target array with shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m was passed for an output of shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(shape)\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while using as loss `\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;241m+\u001b[39m loss_name\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis loss expects targets to have the same shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas the output.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (5, 26) was passed for an output of shape (None, 39) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"/Users/yashp/OneDrive/Desktop/cocktail.csv\")\n",
    "\n",
    "# Extract the relevant columns from the dataset\n",
    "ingredients = data[\"ingredients\"].tolist()\n",
    "instructions = data[\"instructions\"].tolist()\n",
    "\n",
    "# Tokenize the ingredients and instructions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(ingredients + instructions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "ingredients_seq = tokenizer.texts_to_sequences(ingredients)\n",
    "instructions_seq = tokenizer.texts_to_sequences(instructions)\n",
    "\n",
    "# Pad the sequences so that they are all the same length\n",
    "max_length = max([len(seq) for seq in instructions_seq])\n",
    "ingredients_seq = pad_sequences(ingredients_seq, padding=\"post\", maxlen=max_length)\n",
    "instructions_seq = pad_sequences(instructions_seq, padding=\"post\", maxlen=max_length)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(ingredients_seq, instructions_seq, epochs=50, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Generate new cocktail recipes\n",
    "input_ingredients = [\"vodka\", \"cranberry juice\", \"lime juice\"]\n",
    "input_seq = tokenizer.texts_to_sequences([input_ingredients])[0]\n",
    "input_seq = pad_sequences([input_seq], padding=\"post\", maxlen=max_length)\n",
    "output_seq = model.predict_classes(input_seq)[0]\n",
    "output_recipe = tokenizer.sequences_to_texts([output_seq])[0]\n",
    "\n",
    "print(f\"Input ingredients: {', '.join(input_ingredients)}\")\n",
    "print(f\"Generated recipe: {output_recipe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979254e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'mask_rcnn_coco.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load the pre-trained Mask R-CNN model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m seg \u001b[38;5;241m=\u001b[39m instance_segmentation()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mseg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask_rcnn_coco.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Iterate over the images in the input folder\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_folder):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pixellib\\instance\\__init__.py:65\u001b[0m, in \u001b[0;36minstance_segmentation.load_model\u001b[1;34m(self, model_path, confidence)\u001b[0m\n\u001b[0;32m     61\u001b[0m     coco_config\u001b[38;5;241m.\u001b[39mDETECTION_MIN_CONFIDENCE \u001b[38;5;241m=\u001b[39m confidence\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m MaskRCNN(mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, config \u001b[38;5;241m=\u001b[39m coco_config)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pixellib\\instance\\mask_rcnn.py:2095\u001b[0m, in \u001b[0;36mMaskRCNN.load_weights\u001b[1;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`load_weights` requires h5py.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2095\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   2096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_names\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mattrs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m   2097\u001b[0m         f \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    525\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    527\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    528\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    529\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    530\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    531\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    532\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 533\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    225\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 226\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    228\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'mask_rcnn_coco.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pixellib\n",
    "from pixellib.instance import instance_segmentation\n",
    "\n",
    "input_folder = \"/Users/yashp/OneDrive/Desktop/RESEARCH/combined/train/good/\"\n",
    "output_folder = \"/Users/yashp/OneDrive/Desktop/RESEARCH/Segmentation/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Load the pre-trained Mask R-CNN model\n",
    "seg = instance_segmentation()\n",
    "seg.load_model(\"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Iterate over the images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Perform instance segmentation\n",
    "        result = seg.segmentImage(image)\n",
    "\n",
    "        # Get the main object mask\n",
    "        main_object_mask = result[0]['masks'][:, :, 0]\n",
    "\n",
    "        # Apply the mask to the original image\n",
    "        output_image = cv2.bitwise_and(image, image, mask=main_object_mask)\n",
    "\n",
    "        # Save the output image in the output folder\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        cv2.imwrite(output_path, output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d31575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/combined/train/good/'\n",
    "output_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/Segmentation/'\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Load the image\n",
    "    img = cv2.imread(input_dir + filename)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the image\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Loop through all contours\n",
    "    for i, contour in enumerate(contours):\n",
    "        # Get the bounding rectangle\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Crop the contour and save it as a new image\n",
    "        crop_img = img[y:y+h, x:x+w]\n",
    "        cv2.imwrite(output_dir + filename.split('.')[0] + '_' + str(i) + '.jpg', crop_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f42cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/combined/train/good/'\n",
    "output_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/Segmentation/'\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Load the image\n",
    "    img = cv2.imread(input_dir + filename)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the image\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create a black mask with the same size as the original image\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Loop through all contours\n",
    "    for i, contour in enumerate(contours):\n",
    "        # Draw the contour on the mask\n",
    "        cv2.drawContours(mask, [contour], 0, 255, -1)\n",
    "\n",
    "        # Copy the masked region from the original image to a new image with the same size as the original\n",
    "        masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cropped_img = masked_img[y:y+h, x:x+w]\n",
    "        new_img = np.zeros_like(img)\n",
    "        new_img[y:y+h, x:x+w] = cropped_img\n",
    "\n",
    "        # Save the new image to the output directory\n",
    "        cv2.imwrite(output_dir + filename.split('.')[0] + '_' + str(i) + '.jpg', new_img)\n",
    "\n",
    "        # Reset the mask\n",
    "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b09a38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/combined/train/good/'\n",
    "output_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/newsegment'\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Load the image\n",
    "    img = cv2.imread(input_dir + filename)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the image\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw the contours on the original image\n",
    "    cv2.drawContours(img, contours, -1, (0, 0, 255), 2)\n",
    "\n",
    "    # Save the original image to the output directory\n",
    "    cv2.imwrite(output_dir + filename, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14b414e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\drawing.cpp:2605: error: (-215:Assertion failed) reader.ptr != NULL in function 'cvDrawContours'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Draw the main contour on the mask\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawContours\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmax_contour\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Copy the masked region from the original image to a new image with the same size as the original\u001b[39;00m\n\u001b[0;32m     41\u001b[0m masked_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbitwise_and(img, img, mask\u001b[38;5;241m=\u001b[39mmask)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\drawing.cpp:2605: error: (-215:Assertion failed) reader.ptr != NULL in function 'cvDrawContours'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/combined/train/good/'\n",
    "output_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/segment2/'\n",
    "\n",
    "# Create a background subtractor object\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Load the image\n",
    "    img = cv2.imread(input_dir + filename)\n",
    "\n",
    "    # Apply background subtraction to get the foreground mask\n",
    "    fgMask = backSub.apply(img)\n",
    "\n",
    "    # Threshold the foreground mask to create a binary mask\n",
    "    thresh = cv2.threshold(fgMask, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Find contours in the binary mask\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area (i.e., the main object)\n",
    "    max_area = 0\n",
    "    max_contour = None\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_contour = contour\n",
    "\n",
    "    # Create a black mask with the same size as the original image\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Draw the main contour on the mask\n",
    "    cv2.drawContours(mask, [max_contour], 0, 255, -1)\n",
    "\n",
    "    # Copy the masked region from the original image to a new image with the same size as the original\n",
    "    masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "    new_img = np.zeros_like(img)\n",
    "    x, y, w, h = cv2.boundingRect(max_contour)\n",
    "    new_img[y:y+h, x:x+w] = masked_img[y:y+h, x:x+w]\n",
    "\n",
    "    # Save the new image to the output directory\n",
    "    cv2.imwrite(output_dir + filename, new_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88ce4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/combined/test/anomaly/'\n",
    "output_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/segment2/test/anomaly/'\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Load the image\n",
    "    img = cv2.imread(input_dir + filename)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the image\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Check if any contours were found\n",
    "    if len(contours) > 0:\n",
    "        # Draw the contours on the original image\n",
    "        cv2.drawContours(img, contours, -1, (0, 0, 255), 2)\n",
    "\n",
    "    # Save the original image to the output directory\n",
    "    cv2.imwrite(output_dir + filename, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f34d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input directory\n",
    "input_dir = '/Users/yashp/OneDrive/Desktop/RESEARCH/segment2/train/good/'\n",
    "\n",
    "# Loop through all images in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Load the image\n",
    "    img = cv2.imread(input_dir + filename)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold the image\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\n",
    "    # Set the thresholded pixels to black\n",
    "    img[thresh == 255] = 0\n",
    "\n",
    "    # Save the image to the same directory with the prefix 'black_'\n",
    "    cv2.imwrite(input_dir + 'black_' + filename, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033e9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
